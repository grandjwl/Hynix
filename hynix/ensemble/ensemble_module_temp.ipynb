{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics import R2Score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from pycaret.regression import *\n",
    "    \n",
    "test = pd.read_csv(\"C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/dataset/testset.csv\")\n",
    "\n",
    "def get_predictions(test):\n",
    "    \n",
    "    # 동연\n",
    "    torch.manual_seed(1234)\n",
    "    torch.cuda.manual_seed(1234)\n",
    "    torch.cuda.manual_seed_all(1234)\n",
    "    np.random.seed(1234)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(1234)\n",
    "\n",
    "    class DataPreprocessor:\n",
    "        def __init__(self):\n",
    "            self.deleted_columns = None\n",
    "            self.null_columns = None\n",
    "            self.to_drop_all = None\n",
    "\n",
    "        def sort_time(self, data):\n",
    "            for idx,col in enumerate(data.columns[188:1454]):\n",
    "                if data[col].dtype == \"object\":\n",
    "                    data[col] = pd.to_datetime(data[col])\n",
    "            ts_data = data.select_dtypes(\"datetime\")\n",
    "\n",
    "            for idx in ts_data.index:\n",
    "                ts_data.sort_values(by=idx,axis=1, inplace=True)\n",
    "            result = []\n",
    "            datatmp = data.columns.to_list()[188:1454]\n",
    "            for idx,col in enumerate(datatmp):\n",
    "                if data[col].dtype == \"<M8[ns]\":\n",
    "                    cur = int(col[1:]) # x195 -> 195\n",
    "                    i = idx\n",
    "                    tmp = []\n",
    "                    while i > 0:\n",
    "                        i -= 1\n",
    "                        next = datatmp[i] # x194\n",
    "                        if data[next].dtype == \"<M8[ns]\":\n",
    "                            break\n",
    "                        else:\n",
    "                            tmp.append(next)\n",
    "                            tmp.sort()\n",
    "                    result.append((col,tmp))\n",
    "            ts_final = []\n",
    "            for elem in ts_data.columns:\n",
    "                for target,content in result:\n",
    "                    if elem == target:\n",
    "                        ts_final.extend(content)\n",
    "                        ts_final.append(target)\n",
    "            ts_final = data[ts_final]\n",
    "            front = data.loc[:,:\"x193\"]\n",
    "            back = data.loc[:,\"x1461\":]\n",
    "            data = pd.concat([front, ts_final, back], axis = 1)\n",
    "            return data\n",
    "\n",
    "        def Qtime(self, data, ts_data):\n",
    "            df = pd.DataFrame(index=data.index)\n",
    "            for idx in range(1, len(ts_data.columns)):\n",
    "                col = []\n",
    "                for jdx in range(len(ts_data.index)):\n",
    "                    time1 = datetime.strptime(ts_data.iloc[jdx,idx],\"%Y-%m-%d %H:%M:%S\")\n",
    "                    time2 = datetime.strptime(ts_data.iloc[jdx,idx-1],\"%Y-%m-%d %H:%M:%S\")\n",
    "                    diff =  time1 - time2\n",
    "                    col.append(round(diff.seconds/(60*60),2))\n",
    "                df[ts_data.columns[idx]+\"-\"+ts_data.columns[idx-1]] = col\n",
    "            with open('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/train_q_train.pkl', 'wb') as f:\n",
    "                pickle.dump(df, f)\n",
    "            return df\n",
    "\n",
    "        def delete_null1(self, final):\n",
    "            empty_columns = final.columns[final.isnull().all()]\n",
    "            final = final.drop(empty_columns, axis=1)\n",
    "            self.deleted_columns = list(empty_columns)\n",
    "            with open('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/deleted_columns.pkl', 'wb') as f:\n",
    "                pickle.dump(self.deleted_columns, f)\n",
    "            return final, self.deleted_columns\n",
    "\n",
    "        def delete_null2(self, final):\n",
    "            null_threshold = 0.95 \n",
    "            null_counts = final.isnull().sum() \n",
    "            total_rows = final.shape[0]\n",
    "            self.null_columns = null_counts[null_counts / total_rows >= null_threshold].index.tolist()\n",
    "            final = final.drop(self.null_columns, axis=1)\n",
    "            with open('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/null_columns.pkl', 'wb') as f:\n",
    "                pickle.dump(self.null_columns, f)    \n",
    "            return final, self.null_columns\n",
    "        \n",
    "        def fillna_null(self, final):\n",
    "            final.drop(final.columns[0], axis=1, inplace = True)\n",
    "            null_columns = final.columns[final.isnull().any()]\n",
    "            for column in null_columns:\n",
    "                noise = np.random.normal(loc=0, scale=0.01, size=final[column].isnull().sum())\n",
    "                final.loc[final[column].isnull(), column] = noise\n",
    "            return final\n",
    "\n",
    "        def drop_corrfeature(self, final):\n",
    "            corr_matrix_all = final.corr().abs()\n",
    "            upper_all = corr_matrix_all.where(np.triu(np.ones(corr_matrix_all.shape), k=1).astype(bool))\n",
    "            to_drop_all = [column for column in upper_all.columns if any(upper_all[column] > 0.8)]\n",
    "            self.to_drop_all = [column for column in upper_all.columns if any(upper_all[column] > 0.8)]\n",
    "            final = final.drop(self.to_drop_all, axis=1)\n",
    "            with open('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/to_drop_all.pkl', 'wb') as f:\n",
    "                pickle.dump(self.to_drop_all, f) \n",
    "            return final, self.to_drop_all\n",
    "        \n",
    "        def preprocessing_train(self, data):\n",
    "            a = self.sort_time(data)\n",
    "            train_ts_data = a.select_dtypes(\"datetime\").astype(\"str\")\n",
    "            train_q = self.Qtime(a,train_ts_data)\n",
    "            final, deleted_columns = self.delete_null1(data)\n",
    "            final, null_columns = self.delete_null2(final)\n",
    "            final = self.fillna_null(final)\n",
    "            final, to_drop_all = self.drop_corrfeature(final)\n",
    "            final = final.sort_index()\n",
    "            final =  final.select_dtypes(include=['float64'])\n",
    "            final = pd.concat([final,train_q],axis=1)\n",
    "            return final, to_drop_all, deleted_columns, null_columns\n",
    "        \n",
    "        def preprocessing_test(self, data):\n",
    "            a = self.sort_time(data)\n",
    "            train_ts_data = a.select_dtypes(\"datetime\").astype(\"str\")\n",
    "            train_q = self.Qtime(a,train_ts_data)\n",
    "            final = data.drop(self.deleted_columns, axis=1)\n",
    "            final = final.drop(self.null_columns, axis=1)\n",
    "            final = self.fillna_null(final)\n",
    "            final = final.drop(self.to_drop_all, axis=1)\n",
    "            final = final.sort_index()\n",
    "            final =  final.select_dtypes(include=['float64'])\n",
    "            final = pd.concat([final,train_q],axis=1)\n",
    "            return final\n",
    "        \n",
    "        def load_pickles(self):\n",
    "            with open(\"C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/deleted_columns.pkl\", \"rb\") as file:\n",
    "                deleted_columns = pickle.load(file)\n",
    "            with open(\"C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/null_columns.pkl\", \"rb\") as file:\n",
    "                null_columns = pickle.load(file)\n",
    "            with open(\"C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/to_drop_all.pkl\", \"rb\") as file:\n",
    "                to_drop_all = pickle.load(file)\n",
    "            return deleted_columns, null_columns, to_drop_all\n",
    "        \n",
    "        def preprocessing_realtest(self, data):\n",
    "            deleted_columns, null_columns, to_drop_all = DataPreprocessor.load_pickles(self)\n",
    "            a = self.sort_time(data)\n",
    "            train_ts_data = a.select_dtypes(\"datetime\").astype(\"str\")\n",
    "            train_q = self.Qtime(a,train_ts_data)\n",
    "            final = data.drop(deleted_columns, axis=1)\n",
    "            final = final.drop(null_columns, axis=1)\n",
    "            final = self.fillna_null(final)\n",
    "            final = final.drop(to_drop_all, axis=1)\n",
    "            final = final.sort_index()\n",
    "            final =  final.select_dtypes(include=['float64'])\n",
    "            final = pd.concat([final,train_q],axis=1)\n",
    "            return final\n",
    "        \n",
    "        def scale_data(self, temp):\n",
    "            scaler = MinMaxScaler()\n",
    "            final_temp = pd.DataFrame(temp['Y'])\n",
    "            temp = temp.drop('Y', axis=1)\n",
    "            data_preprocessed_scaled = scaler.fit_transform(temp)\n",
    "            data_preprocessed_scaled = pd.DataFrame(data_preprocessed_scaled, columns=temp.columns[:], index=temp.index)\n",
    "            data_preprocessed_scaled['Y'] = final_temp['Y'] / 100 \n",
    "            return data_preprocessed_scaled\n",
    "        \n",
    "        def scale_data_without_target(self, temp):\n",
    "            scaler = MinMaxScaler()\n",
    "            data_preprocessed_scaled = scaler.fit_transform(temp)\n",
    "            data_preprocessed_scaled = pd.DataFrame(data_preprocessed_scaled, columns=temp.columns[:], index=temp.index)\n",
    "            return data_preprocessed_scaled\n",
    "        \n",
    "    class LSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "            super(LSTM, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.dropout = nn.Dropout(dropout_rate)\n",
    "            self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "            out, _ = self.lstm1(x, (h0, c0))\n",
    "            out = self.dropout(out)\n",
    "            out, _ = self.lstm2(out)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "        \n",
    "    test1 = test\n",
    "    with open(\"C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/preprocess_funcs.pkl\", \"rb\") as file:\n",
    "        pf = pickle.load(file)\n",
    "\n",
    "    deleted_columns, null_columns, to_drop_all = pf.load_pickles()\n",
    "    final_test = pf.preprocessing_realtest(test1)\n",
    "    scaled_final_test = pf.scale_data_without_target(final_test)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    with open(\"C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/LSTM.pkl\", \"rb\") as file:\n",
    "        LSTM = pickle.load(file)\n",
    "\n",
    "    model = torch.load('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/kdy/3. final_best_model.pt')\n",
    "    model.eval()\n",
    "\n",
    "    test_tensor = torch.tensor(scaled_final_test.values).float().to(device)\n",
    "    test_tensor = test_tensor.view(-1, 1, 1285)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_tensor)\n",
    "\n",
    "    predictions_test = predictions.cpu().numpy()\n",
    "    predictions_test = predictions_test * 100\n",
    "\n",
    "    kdy_pred = pd.DataFrame(predictions_test) # 833 rows × 1 columns\n",
    "    \n",
    "    #-------------------------------------------------------------------\n",
    "    # 고운\n",
    "\n",
    "    torch.manual_seed(1234)\n",
    "    torch.cuda.manual_seed(1234)\n",
    "    torch.cuda.manual_seed_all(1234)\n",
    "    np.random.seed(1234)\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(1234)\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    class PreprocessAndPredict():\n",
    "        class RealTestDataset(Dataset):\n",
    "            def __init__(self, df):\n",
    "                self.df = df.reset_index(drop=True)\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.df)\n",
    "                \n",
    "            def __getitem__(self,index):\n",
    "                x = self.df.iloc[index, :].values\n",
    "                data = {}\n",
    "                data[\"x\"] = x\n",
    "                return data\n",
    "            \n",
    "        def __init__(self):\n",
    "            pass\n",
    "        def sort_time(self, data):\n",
    "            for idx,col in enumerate(data.columns):\n",
    "                if data[col].dtype == \"object\":\n",
    "                    data[col] = pd.to_datetime(data[col])\n",
    "\n",
    "            ts_data = data.select_dtypes(\"datetime\")\n",
    "\n",
    "            for idx in ts_data.index:\n",
    "                ts_data.sort_values(by=idx,axis=1, inplace=True)\n",
    "\n",
    "            result = []\n",
    "            datatmp = data.columns.to_list()[188:1454]\n",
    "            for idx,col in enumerate(datatmp):\n",
    "                if data[col].dtype == \"<M8[ns]\":\n",
    "                    cur = int(col[1:]) # x195 -> 195\n",
    "                    i = idx\n",
    "                    tmp = []\n",
    "                    while i > 0:\n",
    "                        i -= 1\n",
    "                        next = datatmp[i] # x194\n",
    "                        if data[next].dtype == \"<M8[ns]\":\n",
    "                            break\n",
    "                        else:\n",
    "                            tmp.append(next)\n",
    "                            tmp.sort()\n",
    "                    result.append((col,tmp))\n",
    "            ts_final = []\n",
    "            for elem in ts_data.columns:\n",
    "                for target,content in result:\n",
    "                    if elem == target:\n",
    "                        ts_final.extend(content)\n",
    "                        ts_final.append(target)\n",
    "            ts_final = data[ts_final]\n",
    "            front = data.loc[:,:\"x193\"]\n",
    "            back = data.loc[:,\"x1461\":]\n",
    "            final = pd.concat([front, ts_final, back], axis = 1)\n",
    "\n",
    "            return final\n",
    "\n",
    "        def Qtime(self, data, ts_data):\n",
    "            df = pd.DataFrame(index=data.index)\n",
    "            for idx in range(1, len(ts_data.columns)):\n",
    "                col = []\n",
    "                for jdx in range(len(ts_data.index)):\n",
    "                    try:\n",
    "                        time1 = datetime.strptime(ts_data.iloc[jdx,idx],\"%Y-%m-%d %H:%M\")\n",
    "                        time2 = datetime.strptime(ts_data.iloc[jdx,idx-1],\"%Y-%m-%d %H:%M\")\n",
    "                    except:\n",
    "                        time1 = datetime.strptime(ts_data.iloc[jdx,idx],\"%Y-%m-%d %H:%M:%S\")\n",
    "                        time2 = datetime.strptime(ts_data.iloc[jdx,idx-1],\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                    diff =  time1 - time2\n",
    "                    col.append(round(diff.seconds/(60*60),2))\n",
    "                df[ts_data.columns[idx]+\"-\"+ts_data.columns[idx-1]] = col\n",
    "            return df\n",
    "\n",
    "        def insert_Qtime(self, data, data_q):\n",
    "            idx = 0\n",
    "            for col in data.columns:\n",
    "                try:\n",
    "                    if data.loc[:,col].dtype == \"datetime64[ns]\":\n",
    "                        data.loc[:,col] = data_q.iloc[:,idx].values\n",
    "                        idx += 1\n",
    "                except:\n",
    "                    break\n",
    "            data.drop(columns=\"x197\",inplace=True)\n",
    "            return data\n",
    "\n",
    "        def train_preprocess(self, train):\n",
    "            print(\"train preprocess start\")\n",
    "            train.set_index(keys=\"ID\",inplace=True)\n",
    "            train.drop(columns=\"x204\",inplace=True)\n",
    "            y_train = train[\"Y\"]\n",
    "            train = train.drop(columns=\"Y\")\n",
    "            \n",
    "            train_options = {}\n",
    "            \n",
    "            train = self.sort_time(train)\n",
    "            ts_train = train.select_dtypes(\"datetime\").astype(\"str\")\n",
    "            train_q = self.Qtime(train, ts_train)\n",
    "            train = self.insert_Qtime(train, train_q)\n",
    "            \n",
    "            head = train.loc[:,:\"x193\"]\n",
    "            mid = train.loc[:,\"x205\":\"x196\"]\n",
    "            tail = train[\"x1548\"]\n",
    "            \n",
    "            for elem in head.columns:\n",
    "                if head[elem].notnull().sum() < 5:\n",
    "                    head.drop(columns=elem,inplace=True)\n",
    "\n",
    "            sw = []\n",
    "            sw_pvalues = []\n",
    "\n",
    "            for col in head.columns:\n",
    "                x = head[head[col].notnull()][col]\n",
    "\n",
    "                test_stat, p_val = stats.shapiro(x)\n",
    "                sw.append((col,p_val))\n",
    "                sw_pvalues.append(p_val)\n",
    "\n",
    "            no_cols = []\n",
    "            for col,val in sw:\n",
    "                if val < 0.05:\n",
    "                    no_cols.append(col)\n",
    "\n",
    "            y_cols = []\n",
    "            for col,val in sw:\n",
    "                if val >= 0.05:\n",
    "                    y_cols.append(col)\n",
    "            head = head[y_cols]\n",
    "\n",
    "            nulldf = head.isnull().copy()\n",
    "            for col in head.columns:\n",
    "                for row in head.index:\n",
    "                    if nulldf.loc[row,col] == True:\n",
    "                        head.loc[row,col] = head[col].mean()+np.random.randn()\n",
    "\n",
    "            for col in mid.columns:\n",
    "                mid[col].fillna(mid[col].mean(), inplace=True)\n",
    "\n",
    "            train = pd.concat([head,mid,tail],axis=1)\n",
    "            train_options[\"before_scale_columns\"] = train.columns.to_list()\n",
    "            \n",
    "            std = StandardScaler()\n",
    "            std.fit(train)\n",
    "\n",
    "            train_sc = std.transform(train)\n",
    "            train = pd.DataFrame(data=train_sc, index=train.index, columns=train.columns)\n",
    "\n",
    "            pickle.dump(std, open('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/cgw/std_scaler.pkl', 'wb'))\n",
    "            \n",
    "            corr_df = train.apply(lambda x: x.corr(y_train))\n",
    "            corr_df = corr_df.apply(lambda x: round(x ,2))\n",
    "            df = pd.DataFrame(corr_df[corr_df<1], columns=['corr'])\n",
    "            cols = df[abs(df[\"corr\"]) >= 0.05].index.to_list()\n",
    "            train = train[cols]\n",
    "            \n",
    "            train_options[\"column_names\"] = train.columns.to_list()\n",
    "            train_options[\"column_means\"] = list(train.mean().values)\n",
    "\n",
    "            pickle.dump(train_options, open('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/cgw/train_options.pkl', 'wb'))\n",
    "            \n",
    "            y_train /= 100\n",
    "            train = pd.merge(train, y_train,how=\"left\",on=\"ID\") \n",
    "\n",
    "            return train\n",
    "\n",
    "        def test_preprocess(self, test):\n",
    "            print(\"test preprocess start\")\n",
    "            test.set_index(keys=\"ID\",inplace=True)\n",
    "            test.drop(columns=\"x204\",inplace=True)\n",
    "            \n",
    "            test = self.sort_time(test)\n",
    "            ts_test = test.select_dtypes(\"datetime\").astype(\"str\")\n",
    "            test_q = self.Qtime(test, ts_test)\n",
    "            test = self.insert_Qtime(test, test_q)\n",
    "            \n",
    "            train_options = pickle.load(open('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/cgw/train_options.pkl', 'rb'))\n",
    "            scaler = pickle.load(open('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/cgw/std_scaler.pkl', 'rb'))\n",
    "            \n",
    "            test = test[train_options[\"before_scale_columns\"]]\n",
    "            \n",
    "            test_sc = scaler.transform(test)\n",
    "            test = pd.DataFrame(data=test_sc, index=test.index, columns=test.columns)\n",
    "            \n",
    "            test = test[train_options[\"column_names\"]]\n",
    "            \n",
    "            mean_values = train_options[\"column_means\"]\n",
    "            for idx, col in enumerate(test.columns.to_list()):\n",
    "                test[col].fillna(mean_values[idx], inplace=True)\n",
    "            \n",
    "            test = test[train_options[\"column_names\"]]\n",
    "            \n",
    "            return test\n",
    "\n",
    "        def run(self, data):\n",
    "            print(\"start test\")\n",
    "            test = self.test_preprocess(data)\n",
    "            test = self.RealTestDataset(test)\n",
    "            test_loader = DataLoader(test, batch_size=833, shuffle=False, drop_last=False)\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            model = torch.load('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/cgw/lstm_best_model_sgd_cosine.pt')\n",
    "            \n",
    "            outputs = []\n",
    "            real = []\n",
    "            for data in test_loader:\n",
    "                x = data[\"x\"].to(device)\n",
    "                x = x.to(torch.float)\n",
    "\n",
    "                output = model(x)\n",
    "                output = torch.flatten(output, 0)\n",
    "\n",
    "                outputs.append(output)\n",
    "\n",
    "            outputs = torch.cat(outputs, dim=0) * 100\n",
    "            return outputs\n",
    "        \n",
    "    test2 = test.iloc[:, 1:]\n",
    "    ppp = PreprocessAndPredict()\n",
    "    cgw_pred = ppp.run(test2)\n",
    "    cgw_pred = pd.DataFrame(cgw_pred.detach().numpy())\n",
    "    #-------------------------------------------------------------------\n",
    "    # 정우\n",
    "\n",
    "    class Preprocessor :\n",
    "        default_path = 'C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/ljw'\n",
    "\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def time_preprocessing(self,df):\n",
    "            df.set_index(keys=\"ID\", inplace=True)\n",
    "            df.drop(columns=\"x204\",inplace=True)\n",
    "            for idx,col in enumerate(df.columns.to_list()[188:1454]):\n",
    "                if df[col].dtype == \"object\":\n",
    "                    df[col] = pd.to_datetime(df[col])\n",
    "            datatmp = df.columns.to_list()[188:1454]\n",
    "            ts_data = df.select_dtypes(\"datetime\")\n",
    "            for idx in ts_data.index:\n",
    "                ts_data.sort_values(by=idx,axis=1, inplace=True)\n",
    "\n",
    "            columns = ts_data.columns\n",
    "            num_columns = len(columns)\n",
    "            for i in range(1, num_columns):\n",
    "                column_diff = pd.to_datetime(df.iloc[:, i]) - pd.to_datetime(ts_data.iloc[:, i-1])\n",
    "                column_diff_hours = column_diff.dt.total_seconds() / 3600\n",
    "                ts_data.iloc[:, i] = column_diff_hours\n",
    "\n",
    "            for i in range(len(ts_data.columns)-1):\n",
    "                ts_data.iloc[:, i] = ts_data.iloc[:, i+1]\n",
    "            result = []\n",
    "            for idx,col in enumerate(datatmp):\n",
    "                if df[col].dtype == \"<M8[ns]\":\n",
    "                    cur = int(col[1:])\n",
    "                    i = idx\n",
    "                    tmp = []\n",
    "                    while i > 0:\n",
    "                        i -= 1\n",
    "                        next = datatmp[i]\n",
    "                        if df[next].dtype == \"<M8[ns]\":\n",
    "                            break\n",
    "                        else:\n",
    "                            tmp.append(next)\n",
    "                            tmp.sort()\n",
    "                    result.append((col,tmp))\n",
    "            ts_final = []\n",
    "            for elem in ts_data.columns:\n",
    "                for target,content in result:\n",
    "                    if elem == target:\n",
    "                        ts_final.extend(content)\n",
    "                        ts_final.append(target)\n",
    "\n",
    "            ts_final = df[ts_final]\n",
    "            front = df.loc[:,:\"x193\"]\n",
    "            back = df.loc[:,\"x1461\":]\n",
    "            final = pd.concat([front, ts_final, back], axis = 1)\n",
    "            ts_data.drop(columns=ts_data.columns[-1], inplace=True)\n",
    "\n",
    "            for col in ts_data.columns:\n",
    "                if col in final.columns:\n",
    "                    final[col] = ts_data[col]\n",
    "            final.drop(['x197'], axis=1, inplace=True)\n",
    "            df = final\n",
    "            return df\n",
    "\n",
    "        def na_ratio_preprocessing_train(self,train):\n",
    "            na_percentage = train.isna().sum() / len(train)\n",
    "            high_na_columns = na_percentage[na_percentage >= 0.9].index\n",
    "            train=train.drop(high_na_columns, axis=1,inplace=False)\n",
    "            with open(Preprocessor.default_path + '/high_na_columns.pkl', 'wb') as f:\n",
    "                pickle.dump(high_na_columns, f)\n",
    "            return train\n",
    "\n",
    "        def na_ratio_preprocessing_Rtest(self,Rtest):\n",
    "            with open(Preprocessor.default_path + '/high_na_columns.pkl', 'rb') as f:\n",
    "                high_na_columns = pickle.load(f)\n",
    "            Rtest=Rtest.drop(high_na_columns, axis=1,inplace=False)\n",
    "            return Rtest\n",
    "\n",
    "        def correlation_preprocessing_train(self,train):\n",
    "            numeric_columns = train.select_dtypes(include=['int64', 'float64'])\n",
    "            correlation_with_target = numeric_columns.corr()['Y']\n",
    "            columns_with_low_correlation = correlation_with_target[correlation_with_target.abs() < 0.04].index\n",
    "            train.drop(columns_with_low_correlation, axis=1, inplace=True)\n",
    "            with open(Preprocessor.default_path+'/columns_with_low_correlation.pkl', 'wb') as f:\n",
    "                pickle.dump(columns_with_low_correlation, f)\n",
    "            return train\n",
    "\n",
    "        def correlation_preprocessing_Rtest(self,Rtest):\n",
    "            with open(Preprocessor.default_path+'/columns_with_low_correlation.pkl', 'rb') as f:\n",
    "                columns_with_low_correlation = pickle.load(f)\n",
    "            Rtest.drop(columns_with_low_correlation, axis=1, inplace=True)\n",
    "            return Rtest\n",
    "\n",
    "        def replace_outliers_median_preprocessing_train(self,train , threshold=1.5):\n",
    "            all_lower_bound={}\n",
    "            all_upper_bound={}\n",
    "            all_col_median={}\n",
    "            for col in train.columns:\n",
    "                Q1 = train[col].quantile(0.25)\n",
    "                Q3 = train[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                all_lower_bound[col]=lower_bound\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "                all_upper_bound[col]=upper_bound\n",
    "\n",
    "                train_outliers = train[col][(train[col] < lower_bound) | (train[col] > upper_bound)]\n",
    "                col_median = train[col].median()\n",
    "                all_col_median[col]=col_median\n",
    "                train.loc[train_outliers.index, col] = col_median\n",
    "            with open(Preprocessor.default_path+'/all_lower_bound.pkl', 'wb') as f:\n",
    "                pickle.dump(all_lower_bound, f)\n",
    "            with open(Preprocessor.default_path+'/all_upper_bound.pkl', 'wb') as f:\n",
    "                pickle.dump(all_upper_bound, f)\n",
    "            with open(Preprocessor.default_path+'/all_col_median.pkl', 'wb') as f:\n",
    "                pickle.dump(all_col_median, f)\n",
    "            return train\n",
    "\n",
    "        def replace_outliers_median_preprocessing_Rtest(self,Rtest, threshold=1.5):\n",
    "            with open(Preprocessor.default_path+'/all_lower_bound.pkl', 'rb') as f:\n",
    "                all_lower_bound = pickle.load(f)\n",
    "            with open(Preprocessor.default_path+'/all_upper_bound.pkl', 'rb') as f:\n",
    "                all_upper_bound = pickle.load(f)\n",
    "            with open(Preprocessor.default_path+'/all_col_median.pkl', 'rb') as f:\n",
    "                all_col_median = pickle.load(f)\n",
    "\n",
    "            for col in Rtest.columns:\n",
    "                Rtest_outliers = Rtest[col][(Rtest[col] < all_lower_bound[col]) | (Rtest[col] > all_upper_bound[col])]\n",
    "                Rtest.loc[Rtest_outliers.index, col] = all_col_median[col]\n",
    "            return Rtest\n",
    "\n",
    "        def fillna_mean_preprocessing_train(self,train):\n",
    "            col_means = {}\n",
    "            use_col = []\n",
    "\n",
    "            for col in train.columns:\n",
    "                col_mean = train[col][~train[col].isnull()].mean()\n",
    "                if pd.notna(col_mean):\n",
    "                    train[col].fillna(col_mean, inplace=True)\n",
    "                    use_col.append(col)\n",
    "                    if train[col].nunique() == 1:\n",
    "                        use_col.remove(col)\n",
    "                        col_means[col] = col_mean\n",
    "            with open(Preprocessor.default_path+'/col_means.pkl', 'wb') as f:\n",
    "                pickle.dump(col_means, f)\n",
    "            with open(Preprocessor.default_path+'/use_col.pkl', 'wb') as f:\n",
    "                pickle.dump(use_col, f)\n",
    "            return train[use_col]\n",
    "\n",
    "        def fillna_mean_preprocessing_Rtest(self,Rtest):\n",
    "            with open(Preprocessor.default_path+'/col_means.pkl', 'rb') as f:\n",
    "                col_means = pickle.load(f)\n",
    "            with open(Preprocessor.default_path+'/use_col.pkl', 'rb') as f:\n",
    "                use_col = pickle.load(f)\n",
    "\n",
    "            for col in Rtest.columns:\n",
    "                if col in use_col:\n",
    "                    Rtest[col].fillna(col_means[col], inplace=True)\n",
    "            return Rtest[use_col]\n",
    "\n",
    "        def scale_preprocessing_train(self,train):\n",
    "            scaler = MinMaxScaler()\n",
    "            strain = scaler.fit_transform(train)\n",
    "            with open(Preprocessor.default_path+'/scaler.pkl', 'wb') as f:\n",
    "                pickle.dump(scaler, f)\n",
    "            train = pd.DataFrame(strain, columns=train.columns,index=train.index)\n",
    "            return train\n",
    "\n",
    "        def scale_preprocessing_Rtest(self,Rtest):\n",
    "            with open(Preprocessor.default_path+'/scaler.pkl', 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "            sRtest = scaler.transform(Rtest)\n",
    "            Rtest = pd.DataFrame(sRtest, columns=Rtest.columns,index=Rtest.index)\n",
    "            return Rtest\n",
    "\n",
    "        def preprocessing_train(self, train):\n",
    "            train = self.na_ratio_preprocessing_train(train)\n",
    "            train = self.correlation_preprocessing_train(train)\n",
    "            train = train.drop(columns=['Y'])\n",
    "            train = self.replace_outliers_median_preprocessing_train(train)\n",
    "            train = self.fillna_mean_preprocessing_train(train)\n",
    "            train = self.scale_preprocessing_train(train)\n",
    "            return train\n",
    "\n",
    "        def preprocessing_Rtest(self,df):\n",
    "            Rtest = self.time_preprocessing(df)\n",
    "            Rtest = self.na_ratio_preprocessing_Rtest(Rtest)\n",
    "            Rtest = self.correlation_preprocessing_Rtest(Rtest)\n",
    "            Rtest = self.replace_outliers_median_preprocessing_Rtest(Rtest)\n",
    "            Rtest = self.fillna_mean_preprocessing_Rtest(Rtest)\n",
    "            Rtest = self.scale_preprocessing_Rtest(Rtest)\n",
    "            return Rtest\n",
    "        \n",
    "    default_path = 'C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/ljw/'\n",
    "    with open(default_path + \"Preprocessor\", \"rb\") as f:\n",
    "        preprocess = pickle.load(f)\n",
    "            \n",
    "    test3 = test.iloc[:, 1:]\n",
    "    Rtest = preprocess.preprocessing_Rtest(test3)\n",
    "\n",
    "    best_model = load_model('C:/Users/dykim/OneDrive/바탕 화면/공부자료/22, 23 AI 공부/2023 T아카데미 ASAC/기업 프로젝트/5. ensemble/ljw/ML_best_model')\n",
    "    ljw_pred = predict_model(best_model, data=Rtest)['prediction_label']*100\n",
    "    ljw_pred = pd.DataFrame(ljw_pred)\n",
    "        \n",
    "    # ----------------------------------------------------------------------------\n",
    "    # ensemble\n",
    "    \n",
    "    mse_values = [10.74511, 11.8428, 1.9966]\n",
    "    weights = [1/mse for mse in mse_values]\n",
    "    normalized_weights = [weight/sum(weights) for weight in weights]\n",
    "    \n",
    "    kdy_pred = kdy_pred.reset_index(drop=True)\n",
    "    cgw_pred = cgw_pred.reset_index(drop=True)\n",
    "    ljw_pred = ljw_pred.reset_index(drop=True)\n",
    "\n",
    "    ensemble_module = kdy_pred.iloc[:, 0]*normalized_weights[0] + cgw_pred.iloc[:, 0]*normalized_weights[1] + ljw_pred.iloc[:, 0]*normalized_weights[2]\n",
    "    return ensemble_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start test\n",
      "test preprocess start\n",
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      88.671549\n",
       "1      88.738801\n",
       "2      88.765708\n",
       "3      88.539363\n",
       "4      88.884482\n",
       "         ...    \n",
       "828    88.577301\n",
       "829    88.652145\n",
       "830    88.594025\n",
       "831    88.546569\n",
       "832    88.612476\n",
       "Length: 833, dtype: float64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_predictions(test)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hynix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
